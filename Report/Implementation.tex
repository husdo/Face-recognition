\subsection{The Graphical User Interface}

To make our program simple and easy to use, we have created a Graphical User Interface(GUI). To do so we have used the Qt library but the program can 
still be run without a GUI by specifying the training and testing folders in the command line.
The GUI is divided into 3 main parts: the main window, the result dialogBox and the ImageCapture program.

\subsubsection{The main window}

It is the first window to appear when the program is executed. It contains the following functionalities:
\begin{itemize}
 \item A menu to load and save classifiers in a file (the classifier selected in the list of methods will be used) and to run the ImageCapture 
program.

\item A path selection tool to specify where are the training and validation folders. The paths can be modified directly from the text inputs or 
by browsing directories using the "..." button.

\item A live preview widget displaying the image retrieved from the camera. A Rectangle is added to help the user to center his face on the 
picture.
\item An output console embedded in the window.
\item A normalisation checkbox to apply the normalisation of the images or not.
\item A progressBar to make sure that the program is still running while loading the images (unfortunately it was not possible to do the same for the 
training process).
\end{itemize}

\subsubsection{The result dialogBox}
When a new picture is taken by the user, a result window is created. This dialogBox calls the predict function from the selected classifier and 
displays the results to the user (image, predicted label and confidence level).

\subsubsection{The ImageCapture program}
To create our own database easily we have created a short OpenCV program to take pictures. Unlike the main program, we use the OpenCV highGUI instead 
of the Qt library to display the images. By clicking and dragging the mouse we create a square in the image. This square defines the part of the image 
that will be recorded. With this feature, the user can easily adapt the size and the position of the image to match his face. We have also added a 
big cross in the square so that the user can align his eyes with his nose and mouth.
Finally, when the space bar is pressed the picture inside the square is saved to the specified folder.

\subsubsection{The Webcam object}
During the execution different parts of the program need to access the images taken by the camera of the user(live preview, ImageCapture, 
resultDialogBox,...). Sometimes this needs to be done simultaneously so to avoid any conflict we have created an object named "Webcam". This object 
uses the Qt "QThread" and "QMutex" classes to create a stream from the camera and retrieving the images in a new thread. Thus this code is executed 
in the same time as the main program and the images retrieved from the camera can be accessed at any time and simultaneously without any risk of 
conflict (thanks to the mutex).
The use of the thread also allow us the use of the main program (for training and validation) even if the user doesn't have any camera or if there is a 
problem with it. Without the thread the program would stop as soon as a problem with the camera occurs.

\subsection{Dataset}
\subsubsection{Databases used}
During the project, we used different databases :
\begin{itemize}
 \item The database provided by the AT\&T laboratory in Cambridge
\item The Yale database. 

These first two databases were mainly used to carry out the first tests as the images were already prepared (grayscale, right size, faces centered with different illuminations).
\item The caltech database was used to test the preparation of the images and the methods on faces in front of different backgrounds and illuminations without any preprocessing.
\item Databases of faces of the people in the group project. (Personal databases)
\end{itemize}
\subsubsection{Acquisition of our own Dataset}
Dataset used for training :
\begin{itemize}
 \item 3 persons, 100 pictures for each from 1 take.  
\item 3 persons, 100 pictures for each from 1 take.
\item 3 persons, 10 pictures for each from 2 takes.
\item 5 persons, 15 pictures for each from at least 4 takes.
\end{itemize}
We firstly tried to do a little database with only three people selected from the group. We wanted to have the faces exposed to different illuminations and in front of different backgrounds to avoid a recognition valid only under certain conditions. The process was to make different faces (smiling, opening the mouth...) in different places. A video was then taken while moving the face and the computer in a partially lighted room in order to get different illuminations. 

We have broken down our initial dataset into two folders: a training folder and a testing one. Firstly, we respected a 80\%, 20\% partition between the training and validation dataset. However, there was too many images for each person in the training dataset and we experienced overfitting. Indeed, if the training and validation test were the same, results were good. If we took other images for the validation, the results could be less efficient than a random choice ($<$30\% in the first database), or recognise a type of illumination instead of a person.

The most efficient training set was approximately 10-15 images taken in different conditions. 

\subsection{Preprocessing}
\label{Implementation:preprocessing}
It is known that Eigenfaces and Fisherfaces were sensible to rotation. To maximize the recognition, a preprocessing of the faces was made and consisted in getting a centered, straight head in a known image size. However the Haar detection can take up to 1 second, and the rotation and cropping on the initial colour image up to 0.3 seconds. (The colour image was kept in order to use the image on other methods not yet implemented in the project). The time of processing prevents doing efficient real time recognition. The recognition with normalisation of the images on a 3 person database jumped from 21\% to 89\% of faces recognised.

To improve our results, we also tried to equalize the histogram of the gray image before the analysis of it. As there was no improvement of recognition and as it took some processing time, we stopped the equalisation.

\subsection{Methods}
\subsubsection{Structure of the code}

To implement the methods we created three base class. The image handler(images.h), the recogniser (Facial\_Recogniser.h) and the normaliser (normalizer.h).
\begin{itemize}
	\item The $Images$ class encapsulates the images. It can read from a folder or it's possible to add only a $Mat$ file with a label. It also contains the labels for the pictures.
	
	\item The $Facial\_Recognizer$ is a base class. It has multiple virtual functions to make the interfaces universal. These functions are: train, load, save and predict. It has a validation part which tests the classifier with a set of images and saves the result of the validation into a cvs file.
	
	\item normalizer.h
	The normaliser just reads the original image and gives back the normalised one. If the normalisation fails the original picture will be stored.
	
\end{itemize}

\subsubsection{Neural Network}
We have attempted to create a Neural Network based on the methods mentioned in the literature review but there is currently no specific libraries for face recognition using Neural 
Networks and we didn't manage to create our own Neural Network classifier within the deadline.

\subsubsection{Combined classifier}
We recognised difference in classification rate for each classifier. But the methods misclassified for different pictures. Based on that, a new combined classifier was created. The hardest point was to rescale the confidence rate for each method. For equalisation, the confidence rates was converted to a percentage based on earlier tests. Then voting decision making is applied. If the best confidence is less than 60\% the classifier returns that the face is not recognizable.

\subsection{Loading and Saving}
To enable easier loading of the model through the GUI interface, there is only one file to load. It is a merge of two files : An xml file representing the model itself and a csv file representing the linking between labels (integer) and the names associated.
